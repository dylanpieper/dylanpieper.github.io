---
title: "Batch and Compare the Similarity of LLM Responses in R"
author: "Dylan Pieper"
date: "2025-03-08"
draft: true
format: 
  html:
    toc: true
    toc-depth: 3
    code-fold: false
categories: [packages, LLMs]
---

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

R is leading the way for data scientists to make the most of large language models (LLMs). The tidyverse package [ellmer](https://ellmer.tidyverse.org) is a simple but powerful bridge to LLM APIs, from simple chatting to [streaming responses](https://ellmer.tidyverse.org/articles/streaming-async.html), [extracting data](https://ellmer.tidyverse.org/articles/structured-data.html), and [calling functions](https://ellmer.tidyverse.org/articles/tool-calling.html).

![](/media/ellmer.png){fig-alt="R package hex logo featuring a cartoon turquoise elephant with colorful patchwork ears against a vibrant background of multicolored patterned squares, with the name \"ellmer\" prominently displayed." fig-align="center" width="150"}

## Processing Lots of Chats

Data scientists often need to send many prompts to LLMs and process lots of chats—a process often referred to as **batch processing**. Select LLM providers (e.g., [OpenAI](https://platform.openai.com/docs/guides/batch), [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/batch-processing), [Gemini](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#generative-ai-batch-text-drest), and [Mistral](https://mistral.ai/news/batch-api)) allow you to batch process lots of chats using their API. This option requires you wait up to 24 hours for a response but it's around 50% cheaper compared to making multiple requests. See ellmer's [`batch_chat()`](https://ellmer.tidyverse.org/reference/batch_chat.html) to use batch APIs for select providers.

Processing many chats by making multiple requests costs more, but it delivers immediate responses and supports any LLM provider. To process lots of chats by making multiple requests, I created a package called [chatalot](https://dylanpieper.github.io/chatalot/), which extends ellmer and includes additional features such as persistent caching.

![](/media/chatalot-hex.png){fig-alt="R package hex logo featuring a friendly cartoon elephant with a red-horned helmet, a flaming tail, and patchwork clothes with flames and skulls against a plain cream background, with the name \"hellmer\" prominently displayed." fig-align="center" width="150"}

## Sequential vs Parallel Processing

There are two methods you can use for processing lots of chats: **sequential** and **parallel** processing.

**Sequential** processing requests one chat at a time, which is great when providers don't allow parallel processing or have strict rate limits, or when you want to periodically check the responses.

**Parallel** processing requests multiple chats simultaneously, which is great when you want to get lots of responses quickly. To this end, ellmer recently introduced [`parallel_chat()`](https://ellmer.tidyverse.org/reference/parallel_chat.html). However, ellmer doesn't currently support persistent caching, which means you risk data loss and increased cost if the processing is interrupted or fails.

## How to Chat a Lot

Processing lots of chats in R is simple. Install the chatalot package and configure your provider's API key. You can install the package from CRAN `install.packages("chatalot")` or GitHub `pak::pak("dylanpieper/chatalot")`.

I recommend setting your provider's API keys in your user and/or project environment using `usethis::edit_r_environ(scope = c("user", "project")`, which will open a `.Renviron` file where you can add an API key such as `OPENAI_API_KEY=your-key`.

The chatalot functions [`seq_chat()`](https://dylanpieper.github.io/chatalot/reference/seq_chat.html) and [`future_chat()`](https://dylanpieper.github.io/chatalot/reference/future_chat.html) create sequential and parallel chat processors with methods to process the chats and access the results. For example:

```{r}
#| eval: false
library(chatalot)

# Create parallel chat processor
chat <- future_chat(
  "anthropic/claude-3-5-sonnet-latest",
  system_prompt = "reply concisely, one sentence"
)

# Process prompts
result <- chat$process(
  c(
    "What is R?",
    "What is Python?",
    "What is Julia?",
    "What is Rust?"
  )
)

# Access responses
result$texts()
#> [1] "R is a popular programming language and software environment primarily 
#> used for statistical computing, data analysis, and graphical visualization."           
#> [2] "Python is a high-level, interpreted programming language known for its 
#> simple syntax and readability."
#> [3] "Julia is a high-level, high-performance programming language designed 
#> for numerical and scientific computing, combining the ease of use of Python 
#> with the speed of C."
#> [4] "Rust is a systems programming language focused on safety, concurrency, 
#> and performance, developed by Mozilla Research."    
```

## Compare the Similarity of LLM Responses

Let's say we want to conduct a sentiment analysis on lots of texts, which is subjective task where there's no clear right or wrong answer. Normally, like any good scientist, we would ask our research assistants to independently code the sentiment (positive, neutral, or negative). Then, we would analyze the ratings to determine the [inter-rater reliability](https://en.wikipedia.org/wiki/Inter-rater_reliability) and ask a trusted colleague or expert to resolve any conflicts.

But, what if, we simulated this process using different LLMs. For example, just like research assistants, we could ask OpenAI and Claude to complete the same task, compare their responses, and resolve any conflicts. This strategy is often referred to as **LLM-as-a-judge**. Although, in this case, a more accurate term might be **LLM-as-a-rater**. We can even compare the LLM raters with human raters.

That's how I got the idea for [samesies](https://dylanpieper.github.io/samesies/)—a package I developed to compare lists of texts, factors, or numerical values to measure their similarity. You can the package from CRAN: `install.packages("samesies")`. The three primary functions `same_text()`, `same_factor()`, and `same_number()` accept two or more lists as inputs, including nested listed. They are inherently typed and will not work with mixed types. For example:

```{r}
#| eval: false
# Text
r1 <- list(
  "R is a statistical computing software",
  "R enables grammar of graphics using ggplot2",
  "R supports advanced statistical models"
)
r2 <- list(
  "R is a full-stack programming language",
  "R enables advanced data visualizations",
  "R supports machine learning algorithms"
)
tex <- same_text(r1, r2)

# Factors
cats1 <- list("R", "R", "Python")
cats2 <- list("R", "Python", "R")
fct <- same_factor(cats1, cats2,
  levels = c("R", "Python")
)

# Numbers
n1 <- list(1, 2, 3)
n2 <- list(1, 2.1, 3.2)
num <- same_number(n1, n2)
```

![](/media/samesies-hex.png){fig-alt="R pacakge hex logo image of three Spiderman characters from different movies pointing at each other. This is based on a meme that was created from a 1967 episode of the Spider-Man cartoon series, featuring two Spider-Men accusing each other of being impostors. The original image is fan art created by the Reddit user WistlerR15." fig-align="center" width="150"}

## Complete Example

I'll use a minimal example to demonstrate how to evaluate the similarity of LLM responses in R using chatalot and samesies.

I will chat with OpenAI's ChatGPT and Anthropic's Claude and ask for the sentiment of prompts in the following formats:

-   text (emotional tone),
-   factor (positive, neutral, or negative), and
-   number (0.0 to 1.0).

Below are my prompts and code to process the LLM responses:

```{r}
#| eval: false
library(chatalot)

prompts <- list(
  # Positive
  "R makes data visualization incredibly easy with ggplot2.",
  "I love how R integrates statistics and data science seamlessly.",
  "The R community is really supportive and welcoming.",
  # Neutral
  "R is commonly used in academic research.",
  "R has both base functions and tidyverse functions for data manipulation.",
  "RStudio is one of the most popular IDEs for R development.",
  # Negative
  "R is painfully slow for large datasets compared to Python.",
  "R's object-oriented system is confusing and inconsistent.",
  "Installing packages in R can be frustrating due to dependency errors.",
  # Ambiguous
  "I use R every day, but I'm not sure if I love it or hate it.",
  "Tidyverse makes R more accessible, but it adds another layer of abstraction.",
  "R has a steep learning curve, but once you get it, it's great."
)

openai_chat <- future_chat("openai/gpt-4.1")
claude_chat <- future_chat("anthropic/claude-3-5-sonnet-latest")

type_sentiment <- type_object(
  "Extract sentiments",
  sentiment_str = type_string("Describe the emotional tone in one word"),
  sentiment_fct = type_enum(c("positive", "neutral", "negative"), "The sentiment type"),
  sentiment_num = type_number("Negative to positive sentiment score, 0.00 to 1.00")
)

openai_dat <- openai_chat$process(prompts, type = type_sentiment)
claude_dat <- claude_chat$process(prompts, type = type_sentiment)

openai_dat <- openai_dat$texts()
claude_dat <- claude_dat$texts()
```

Next, compare the similarity of the responses:

```{r}
#| eval: false
library(samesies)

check_str <- same_text(
  "openai" = openai_dat$sentiment_str |> as.list(),
  "claude" = claude_dat$sentiment_str |> as.list()
)
average_similarity(check_str)
#>   osa      lv      dl hamming     lcs   qgram  cosine jaccard      jw soundex
#> 0.390   0.390   0.390   0.194   0.219   0.584   0.643   0.495   0.666   0.250

check_fct <- same_factor(
  "openai" = openai_dat$sentiment_fct |> as.list(),
  "claude" = claude_dat$sentiment_fct |> as.list(),
  levels = c("positive", "neutral", "negative")
)
average_similarity(check_fct)
#> exact
#> 0.833

check_num <- same_number(
  "openai" = openai_dat$sentiment_num |> as.list(),
  "claude" = claude_dat$sentiment_num |> as.list()
)
average_similarity(check_num)
#> exact        raw        exp    percent normalized      fuzzy
#> 0.417      0.056      0.948      0.883      0.930      0.950
```

From this analysis, I can conclude that the two models have good agreement on the factor-based sentiment classification (83% exact match), excellent agreement on the numerical sentiment scores (93% normalized similarity), and moderate agreement on the text descriptions (64% cosine similarity). The lower agreement on text descriptions reflects the subjective nature of choosing a single word to describe emotional tone.

## Conclusion

Working with LLMs in R has evolved rapidly, and the combination of chatalot for efficient batching and samesies for response comparison creates a powerful toolkit for data scientists. These packages fill important gaps in the R ecosystem and enable more sophisticated LLM workflows.

The ability to validate LLM outputs against each other provides a level of quality assurance that's necessary for research and production systems. The example demonstrated that models tend to agree strongly on structured outputs, with more variation in free-text responses—a pattern that can help guide how we design extraction prompts.

Looking ahead, these tools will become increasingly important as organizations integrate LLMs into their data pipelines. The R ecosystem continues to lead in providing pragmatic solutions for working with AI, allowing data scientists to leverage these technologies without sacrificing reliability or interpretability.

I hope that these packages can become part of the standard toolkit for anyone working with LLMs in R. The combination of efficient batching and systematic comparison provides the foundation for reliable LLM implementations.
