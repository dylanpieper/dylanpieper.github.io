{
  "hash": "2a8a38b624b2aaea31c5c7d0dfdf68f7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Batch and Compare the Similarity of LLM Responses in R\"\nauthor: \"Dylan Pieper\"\ndate: \"2025-03-08\"\nformat: \n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\ncategories: [packages, LLMs]\n---\n\n\n\n\n\n## Introduction\n\nR is leading the way for data scientists to make the most of large language models. Posit's official package [ellmer](https://ellmer.tidyverse.org) provides a powerful functional interface for chatting, streaming responses, extracting data, and calling functions—built on the modern, safe, and fast HTTP client [httr2](https://httr2.r-lib.org).\n\n![](/media/ellmer.png){fig-alt=\"R package hex logo featuring a cartoon turquoise elephant with colorful patchwork ears against a vibrant background of multicolored patterned squares, with the name \\\"ellmer\\\" prominently displayed.\" fig-align=\"center\" width=\"150\"}\n\n## Processing a Lot of Chats\n\nData scientists often need to provide many prompts to LLMs and process a lot of chats at a time—a process often referred to as **batch processing**. Select LLM providers (e.g., [OpenAI](https://platform.openai.com/docs/guides/batch), [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/batch-processing), [Gemini](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#generative-ai-batch-text-drest), and [Mistral](https://mistral.ai/news/batch-api)) allow you to batch process a lot of chats using their API in what is called asynchronous processing. This option makes you wait up to 24 hours for the provide to deliver your responses but is around 50% cheaper than requesting responses synchronously. See [batch_chat()](https://ellmer.tidyverse.org/reference/batch_chat.html) in ellmer to use batch APIs for select providers.\n\nSynchronous, or real-time, batching costs more, but it delivers immediate responses and supports any LLM provider. To synchronously process a lot of chats, I created a package using ellmer called [chatalot](https://dylanpieper.github.io/chatalot/) with a simple interface and features such as [tool calling](https://ellmer.tidyverse.org/articles/tool-calling.html), [structured data extraction](https://ellmer.tidyverse.org/articles/structured-data.html), save and resume, sounds notifications, and more.\n\n![](/media/chatalot-hex.png){fig-alt=\"R package hex logo featuring a friendly cartoon elephant with a red-horned helmet, a flaming tail, and patchwork clothes with flames and skulls against a plain cream background, with the name \\\"hellmer\\\" prominently displayed.\" fig-align=\"center\" width=\"150\"}\n\n## Sequential vs Parallel Processing\n\nThere are two methods you can use for processing many chats: **sequential** or **parallel** processing.\n\nSequential processing requests one chat at a time. Sequential processing is slow but safe, because you can save each response, one at a time.\n\nParallel processing requests multiple chats at a time across multiple R processes using [future](https://future.futureverse.org). It is fast but must be interrupted to collect and save the responses. That is why the chats are distributed across the processes in chunks (e.g., 10 prompts). Once a chunk is finished, the responses are saved to the disk.\n\nIn the latest version of ellmer (0.2.0), [parallel_chat()](https://ellmer.tidyverse.org/reference/parallel_chat.html) was added to support parallel processing. However, it doesn't currently support save and resume.\n\n## How to Chat a Lot\n\nImplementing synchronous batching in R is simple and only requires that you have the package installed and your provider's API key configured. You can install the package from CRAN: `install.packages(\"chatalot\")`.\n\nI recommend setting your API keys for your LLM providers in my user or project environment using `usethis::edit_r_environ(scope = c(\"user\", \"project\")`, which will open a `.Renviron` file where you can add any API key, such as `OPENAI_API_KEY=your-key` or `ANTHROPIC_API_KEY=your-key`.\n\nThe two primary functions `chat_sequential` and `chat_future` create a sequential or parallel processor around an ellmer chat function. For example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(chatalot)\n\n# Option 1: Sequential processing\nchat <- chat_sequential(chat_openai(system_prompt = \"Reply concisely\"))\n\n# Option 2: Parallel processing via future\nchat <- chat_future(chat_openai(system_prompt = \"Reply concisely\"))\n\nresult <- chat$batch(list(\n  \"What is R?\",\n  \"What is Python?\",\n  \"What is Julia?\",\n  \"What is Rust?\"\n))\n# Methods\nresult$progress() # Return batch progress (if interuppted)\nresult$texts() # Return list of responses\nresult$chats() # Return ellmer chat objects\n```\n:::\n\n\n\n## Compare the Similarity of LLM Responses\n\nLet's say I want to conduct a sentiment analysis on a lot of texts, which is a somewhat subjective task where there's no clear right or wrong answer. Normally, like a good scientist, I would ask my research assistants to independently code the sentiment (positive, neutral, or negative). Then, I would analyze the ratings to determine the [inter-rater reliability](https://en.wikipedia.org/wiki/Inter-rater_reliability) and ask a trusted colleague to resolve any conflicts.\n\nBut, what if, I simulated this process using different LLMs. For example, just like research assistants, I could ask OpenAI and Claude to complete the same task, compare their responses, and resolve any conflicts. This strategy is often referred to as **LLM-as-a-judge**. Although, in this case, a more accurate term might be **LLM-as-a-rater**. I can even compare the LLM raters with human raters.\n\nThat's how I got the idea for [samesies](https://dylanpieper.github.io/samesies/)—a package I developed to compare lists of texts, factors, or numerical values to measure their similarity. You can the package from CRAN: `install.packages(\"samesies\")`. The three primary functions `same_text`, `same_factor`, and `same_number` accept two or more lists as inputs, including nested listed. They are inherently typed and will not work with mixed types. For example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Text\nr1 <- list(\n  \"R is a statistical computing software\",\n  \"R enables grammar of graphics using ggplot2\",\n  \"R supports advanced statistical models\"\n)\nr2 <- list(\n  \"R is a full-stack programming language\",\n  \"R enables advanced data visualizations\",\n  \"R supports machine learning algorithms\"\n)\ntex <- same_text(r1, r2)\n\n# Factors\ncats1 <- list(\"R\", \"R\", \"Python\")\ncats2 <- list(\"R\", \"Python\", \"R\")\nfct <- same_factor(cats1, cats2,\n  levels = c(\"R\", \"Python\")\n)\n\n# Numbers\nn1 <- list(1, 2, 3)\nn2 <- list(1, 2.1, 3.2)\nnum <- same_number(n1, n2)\n```\n:::\n\n\n\n![](/media/samesies-hex.png){fig-alt=\"R pacakge hex logo image of three Spiderman characters - from different movies - pointing at each other. This is based on a meme that was created from a 1967 episode of the Spider-Man cartoon series, featuring two Spider-Men accusing each other of being impostors. The original image is fan art created by the Reddit user WistlerR15.\" fig-align=\"center\" width=\"150\"}\n\n## Complete Example\n\nI'll use a minimal example to demonstrate how to batch and evaluate the similarity of LLM responses in R using chatalot and samesies.\n\nI will chat with OpenAI (gpt-4o) and Claude (claude-3-5-sonnet-latest). I'll ask for the sentiment of our prompts as a:\n\n-   text (emotional tone),\n-   factor (positive, neutral, or negative), and\n-   number (0.0 to 1.0).\n\nBelow are my prompts and code to process the LLM responses:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(chatalot)\n\nprompts <- list(\n  # Positive\n  \"R makes data visualization incredibly easy with ggplot2.\",\n  \"I love how R integrates statistics and data science seamlessly.\",\n  \"The R community is really supportive and welcoming.\",\n  # Neutral\n  \"R is commonly used in academic research.\",\n  \"R has both base functions and tidyverse functions for data manipulation.\",\n  \"RStudio is one of the most popular IDEs for R development.\",\n  # Negative\n  \"R is painfully slow for large datasets compared to Python.\",\n  \"R's object-oriented system is confusing and inconsistent.\",\n  \"Installing packages in R can be frustrating due to dependency errors.\",\n  # Ambiguous\n  \"I use R every day, but I'm not sure if I love it or hate it.\",\n  \"Tidyverse makes R more accessible, but it adds another layer of abstraction.\",\n  \"R has a steep learning curve, but once you get it, it's great.\"\n)\n\nopenai_chat <- chat_future(chat_openai())\nclaude_chat <- chat_future(chat_claude())\n\ntype_sentiment <- type_object(\n  \"Extract sentiments\",\n  sentiment_str = type_string(\"Describe the emotional tone in one word\"),\n  sentiment_fct = type_enum(\"The sentiment type\", c(\"positive\", \"neutral\", \"negative\")),\n  sentiment_num = type_number(\"Negative to positive sentiment score, 0.00 to 1.00\"),\n)\n\nopenai_dat <- openai_chat$lot(prompts, type = type_sentiment)\nclaude_dat <- claude_chat$lot(prompts, type = type_sentiment)\n\nopenai_dat <- openai_dat$texts()\nclaude_dat <- claude_dat$texts()\n```\n:::\n\n\n\nNext, compare the similarity of the responses:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(samesies)\n\ncheck_str <- same_text(\n  \"openai\" = openai_dat$sentiment_str |> as.list(),\n  \"claude\" = claude_dat$sentiment_str |> as.list()\n)\naverage_similarity(check_str)\n#>   osa      lv      dl hamming     lcs   qgram  cosine jaccard      jw soundex\n#> 0.390   0.390   0.390   0.194   0.219   0.584   0.643   0.495   0.666   0.250\n\ncheck_fct <- same_factor(\n  \"openai\" = openai_dat$sentiment_fct |> as.list(),\n  \"claude\" = claude_dat$sentiment_fct |> as.list(),\n  levels = c(\"positive\", \"neutral\", \"negative\")\n)\naverage_similarity(check_fct)\n#> exact\n#> 0.833\n\ncheck_num <- same_number(\n  \"openai\" = openai_dat$sentiment_num |> as.list(),\n  \"claude\" = claude_dat$sentiment_num |> as.list()\n)\naverage_similarity(check_num)\n#> exact        raw        exp    percent normalized      fuzzy\n#> 0.417      0.056      0.948      0.883      0.930      0.950\n```\n:::\n\n\n\nFrom this analysis, I can conclude that the two models have good agreement on the factor-based sentiment classification (83% exact match), excellent agreement on the numerical sentiment scores (93% normalized similarity), and moderate agreement on the text descriptions (64% cosine similarity). The lower agreement on text descriptions reflects the subjective nature of choosing a single word to describe emotional tone.\n\n## Conclusion\n\nWorking with LLMs in R has evolved rapidly, and the combination of chatalot for efficient batching and samesies for response comparison creates a powerful toolkit for data scientists. These packages fill important gaps in the R ecosystem and enable more sophisticated LLM workflows.\n\nThe ability to validate LLM outputs against each other provides a level of quality assurance that's necessary for research and production systems. The example demonstrated that models tend to agree strongly on structured outputs, with more variation in free-text responses—a pattern that can help guide how we design extraction prompts.\n\nLooking ahead, these tools will become increasingly important as organizations integrate LLMs into their data pipelines. The R ecosystem continues to lead in providing pragmatic solutions for working with AI, allowing data scientists to leverage these technologies without sacrificing reliability or interpretability.\n\nI hope that these packages can become part of the standard toolkit for anyone working with LLMs in R. The combination of efficient batching and systematic comparison provides the foundation for reliable LLM implementations.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}