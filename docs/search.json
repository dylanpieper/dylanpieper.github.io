[
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Packages",
    "section": "",
    "text": "REDCap data to a database and use in R without exceeding available memory\n\n\n\nchatalot makes it easy to process a lot of large language model chats using ellmer\n\n\n\nsamesies compares lists of texts, factors, or numerical values to measure their similarity"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nredquack: An R Package for Memory Efficient REDCap-to-DB Workflows\n\n\n\npackages\n\nredcap\n\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\nDylan Pieper\n\n\n\n\n\n\n\n\n\n\n\n\nBatch and Compare the Similarity of LLM Responses in R\n\n\n\npackages\n\nLLMs\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nDylan Pieper\n\n\n\n\n\nNo matching items\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html",
    "href": "blog/posts/batch-and-compare-LLM-responses.html",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "",
    "text": "R is leading the way for data scientists to make the most of large language models. Posit’s official package ellmer provides a powerful functional interface for chatting, streaming responses, extracting data, and calling functions—built on the modern, safe, and fast HTTP client httr2."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#introduction",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#introduction",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "",
    "text": "R is leading the way for data scientists to make the most of large language models. Posit’s official package ellmer provides a powerful functional interface for chatting, streaming responses, extracting data, and calling functions—built on the modern, safe, and fast HTTP client httr2."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#processing-a-lot-of-chats",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#processing-a-lot-of-chats",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Processing a Lot of Chats",
    "text": "Processing a Lot of Chats\nData scientists often need to provide many prompts to LLMs and process a lot of chats at a time—a process often referred to as batch processing. Select LLM providers (e.g., OpenAI, Anthropic, Gemini, and Mistral) allow you to batch process a lot of chats using their API in what is called asynchronous processing. This option makes you wait up to 24 hours for the provide to deliver your responses but is around 50% cheaper than requesting responses synchronously. See batch_chat() in ellmer to use batch APIs for select providers.\nSynchronous, or real-time, batching costs more, but it delivers immediate responses and supports any LLM provider. To synchronously process a lot of chats, I created a package using ellmer called chatalot with a simple interface and features such as tool calling, structured data extraction, save and resume, sounds notifications, and more."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#sequential-vs-parallel-processing",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#sequential-vs-parallel-processing",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Sequential vs Parallel Processing",
    "text": "Sequential vs Parallel Processing\nThere are two methods you can use for processing many chats: sequential or parallel processing.\nSequential processing requests one chat at a time. Sequential processing is slow but safe, because you can save each response, one at a time.\nParallel processing requests multiple chats at a time across multiple R processes using future. It is fast but must be interrupted to collect and save the responses. That is why the chats are distributed across the processes in chunks (e.g., 10 prompts). Once a chunk is finished, the responses are saved to the disk.\nIn the latest version of ellmer (0.2.0), parallel_chat() was added to support parallel processing. However, it doesn’t currently support save and resume."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#how-to-chat-a-lot",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#how-to-chat-a-lot",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "How to Chat a Lot",
    "text": "How to Chat a Lot\nImplementing synchronous batching in R is simple and only requires that you have the package installed and your provider’s API key configured. You can install the package from CRAN: install.packages(\"chatalot\").\nI recommend setting your API keys for your LLM providers in my user or project environment using usethis::edit_r_environ(scope = c(\"user\", \"project\"), which will open a .Renviron file where you can add any API key, such as OPENAI_API_KEY=your-key or ANTHROPIC_API_KEY=your-key.\nThe two primary functions chat_sequential and chat_future create a sequential or parallel processor around an ellmer chat function. For example:\n\nlibrary(chatalot)\n\n# Option 1: Sequential processing\nchat &lt;- chat_sequential(chat_openai(system_prompt = \"Reply concisely\"))\n\n# Option 2: Parallel processing via future\nchat &lt;- chat_future(chat_openai(system_prompt = \"Reply concisely\"))\n\nresult &lt;- chat$batch(list(\n  \"What is R?\",\n  \"What is Python?\",\n  \"What is Julia?\",\n  \"What is Rust?\"\n))\n# Methods\nresult$progress() # Return batch progress (if interuppted)\nresult$texts() # Return list of responses\nresult$chats() # Return ellmer chat objects"
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#compare-the-similarity-of-llm-responses",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#compare-the-similarity-of-llm-responses",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Compare the Similarity of LLM Responses",
    "text": "Compare the Similarity of LLM Responses\nLet’s say I want to conduct a sentiment analysis on a lot of texts, which is a somewhat subjective task where there’s no clear right or wrong answer. Normally, like a good scientist, I would ask my research assistants to independently code the sentiment (positive, neutral, or negative). Then, I would analyze the ratings to determine the inter-rater reliability and ask a trusted colleague to resolve any conflicts.\nBut, what if, I simulated this process using different LLMs. For example, just like research assistants, I could ask OpenAI and Claude to complete the same task, compare their responses, and resolve any conflicts. This strategy is often referred to as LLM-as-a-judge. Although, in this case, a more accurate term might be LLM-as-a-rater. I can even compare the LLM raters with human raters.\nThat’s how I got the idea for samesies—a package I developed to compare lists of texts, factors, or numerical values to measure their similarity. You can the package from CRAN: install.packages(\"samesies\"). The three primary functions same_text, same_factor, and same_number accept two or more lists as inputs, including nested listed. They are inherently typed and will not work with mixed types. For example:\n\n# Text\nr1 &lt;- list(\n  \"R is a statistical computing software\",\n  \"R enables grammar of graphics using ggplot2\",\n  \"R supports advanced statistical models\"\n)\nr2 &lt;- list(\n  \"R is a full-stack programming language\",\n  \"R enables advanced data visualizations\",\n  \"R supports machine learning algorithms\"\n)\ntex &lt;- same_text(r1, r2)\n\n# Factors\ncats1 &lt;- list(\"R\", \"R\", \"Python\")\ncats2 &lt;- list(\"R\", \"Python\", \"R\")\nfct &lt;- same_factor(cats1, cats2,\n  levels = c(\"R\", \"Python\")\n)\n\n# Numbers\nn1 &lt;- list(1, 2, 3)\nn2 &lt;- list(1, 2.1, 3.2)\nnum &lt;- same_number(n1, n2)"
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#complete-example",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#complete-example",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Complete Example",
    "text": "Complete Example\nI’ll use a minimal example to demonstrate how to batch and evaluate the similarity of LLM responses in R using chatalot and samesies.\nI will chat with OpenAI (gpt-4o) and Claude (claude-3-5-sonnet-latest). I’ll ask for the sentiment of our prompts as a:\n\ntext (emotional tone),\nfactor (positive, neutral, or negative), and\nnumber (0.0 to 1.0).\n\nBelow are my prompts and code to process the LLM responses:\n\nlibrary(chatalot)\n\nprompts &lt;- list(\n  # Positive\n  \"R makes data visualization incredibly easy with ggplot2.\",\n  \"I love how R integrates statistics and data science seamlessly.\",\n  \"The R community is really supportive and welcoming.\",\n  # Neutral\n  \"R is commonly used in academic research.\",\n  \"R has both base functions and tidyverse functions for data manipulation.\",\n  \"RStudio is one of the most popular IDEs for R development.\",\n  # Negative\n  \"R is painfully slow for large datasets compared to Python.\",\n  \"R's object-oriented system is confusing and inconsistent.\",\n  \"Installing packages in R can be frustrating due to dependency errors.\",\n  # Ambiguous\n  \"I use R every day, but I'm not sure if I love it or hate it.\",\n  \"Tidyverse makes R more accessible, but it adds another layer of abstraction.\",\n  \"R has a steep learning curve, but once you get it, it's great.\"\n)\n\nopenai_chat &lt;- chat_future(chat_openai())\nclaude_chat &lt;- chat_future(chat_claude())\n\ntype_sentiment &lt;- type_object(\n  \"Extract sentiments\",\n  sentiment_str = type_string(\"Describe the emotional tone in one word\"),\n  sentiment_fct = type_enum(\"The sentiment type\", c(\"positive\", \"neutral\", \"negative\")),\n  sentiment_num = type_number(\"Negative to positive sentiment score, 0.00 to 1.00\"),\n)\n\nopenai_dat &lt;- openai_chat$lot(prompts, type = type_sentiment)\nclaude_dat &lt;- claude_chat$lot(prompts, type = type_sentiment)\n\nopenai_dat &lt;- openai_dat$texts()\nclaude_dat &lt;- claude_dat$texts()\n\nNext, compare the similarity of the responses:\n\nlibrary(samesies)\n\ncheck_str &lt;- same_text(\n  \"openai\" = openai_dat$sentiment_str |&gt; as.list(),\n  \"claude\" = claude_dat$sentiment_str |&gt; as.list()\n)\naverage_similarity(check_str)\n#&gt;   osa      lv      dl hamming     lcs   qgram  cosine jaccard      jw soundex\n#&gt; 0.390   0.390   0.390   0.194   0.219   0.584   0.643   0.495   0.666   0.250\n\ncheck_fct &lt;- same_factor(\n  \"openai\" = openai_dat$sentiment_fct |&gt; as.list(),\n  \"claude\" = claude_dat$sentiment_fct |&gt; as.list(),\n  levels = c(\"positive\", \"neutral\", \"negative\")\n)\naverage_similarity(check_fct)\n#&gt; exact\n#&gt; 0.833\n\ncheck_num &lt;- same_number(\n  \"openai\" = openai_dat$sentiment_num |&gt; as.list(),\n  \"claude\" = claude_dat$sentiment_num |&gt; as.list()\n)\naverage_similarity(check_num)\n#&gt; exact        raw        exp    percent normalized      fuzzy\n#&gt; 0.417      0.056      0.948      0.883      0.930      0.950\n\nFrom this analysis, I can conclude that the two models have good agreement on the factor-based sentiment classification (83% exact match), excellent agreement on the numerical sentiment scores (93% normalized similarity), and moderate agreement on the text descriptions (64% cosine similarity). The lower agreement on text descriptions reflects the subjective nature of choosing a single word to describe emotional tone."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#conclusion",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#conclusion",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with LLMs in R has evolved rapidly, and the combination of chatalot for efficient batching and samesies for response comparison creates a powerful toolkit for data scientists. These packages fill important gaps in the R ecosystem and enable more sophisticated LLM workflows.\nThe ability to validate LLM outputs against each other provides a level of quality assurance that’s necessary for research and production systems. The example demonstrated that models tend to agree strongly on structured outputs, with more variation in free-text responses—a pattern that can help guide how we design extraction prompts.\nLooking ahead, these tools will become increasingly important as organizations integrate LLMs into their data pipelines. The R ecosystem continues to lead in providing pragmatic solutions for working with AI, allowing data scientists to leverage these technologies without sacrificing reliability or interpretability.\nI hope that these packages can become part of the standard toolkit for anyone working with LLMs in R. The combination of efficient batching and systematic comparison provides the foundation for reliable LLM implementations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan Pieper",
    "section": "",
    "text": "I am a data scientist in the School of Pharmacy at the University of Pittsburgh. I build data products and tools for researchers and organizations to ensure that projects succeed. I translate human needs into technical solutions, mainly using R and modern database infrastructure. I have a knack for co-creating products with collaborative teams to help diverse communities thrive.\n\n\nI am also a dog dad 🐾, paragliding pilot 🪂, and hot yogi 🧘.\n\n\nHappy to connect with you, dylanpieper@pitt.edu!"
  },
  {
    "objectID": "blog/posts/redquack-rmed.html",
    "href": "blog/posts/redquack-rmed.html",
    "title": "redquack: An R Package for Memory Efficient REDCap-to-DB Workflows",
    "section": "",
    "text": "At the R/Medicine 2025 conference, I addressed a challenge in healthcare data management with the introduction of the redquack package. Traditional REDCap data extraction using packages like REDCapR often fails when handling large datasets due to memory constraints. At the University of Pittsburgh School of Pharmacy, I work with a REDCap database containing nearly 3 million rows across 400 columns from over 200 outpatient treatment centers, creating scenarios where traditional extraction methods fail. This package solves this problem by leveraging DuckDB’s columnar storage format and implementing a batch processing approach that extracts data in configurable chunks of record IDs, bypassing memory limitations and maintaining integration with tidyverse workflows.\n\n\n\n\n\nUsing the redcap_to_db() function, researchers can efficiently process datasets that exceed hardware memory capacity while preserving familiar dplyr syntax for data manipulation. Key features include automatic column type optimization across batches, comprehensive logging for progress tracking, and user-friendly elements such as sound notifications for long-running operations (a “quack” on success). This solution has proven particularly valuable for complex, multi-form projects and longitudinal studies that were previously not possible. This package represents a scalable solution that enables researchers and data scientists to harness the full potential of big REDCap data.\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2022\n\nDimant, E., Clemente, E. G., Pieper, D., & Azevedo, F. (2022). Politicizing mask-wearing: Predicting the success of behavioral interventions among Republicans and Democrats in the U.S. Scientific Reports, 12, 7575.  doi.org/10.1038/s41598-022-10524-1\n\n\n\n2021\n\nGelfand, M. J., Li, R., Stamkou, E., Denison, E., Fernandez, J., Choi, V., Chatmen, J., Jackson, J. C., Pieper, D., & Dimant, E. (2021). Persuading conservatives and liberals to comply with mask-wearing: An intervention tournament. Journal of Experimental Social Psychology.  doi.org/10.1016/j.jesp.2022.104299\nGelfand, M. J., Jackson, J. C., Pan, X., Nau, D., Pieper, D., Denison, E., Dagher, M., Van Lange, P. A. M., Chiu, C., & Wang, M. (2021). The relationship between cultural tightness–looseness and COVID-19 cases and deaths: A global analysis. The Lancet: Planetary Health.  doi.org/10.1016/S2542-5196(20)30301-6"
  },
  {
    "objectID": "publications.html#published",
    "href": "publications.html#published",
    "title": "Publications",
    "section": "",
    "text": "2022\n\nDimant, E., Clemente, E. G., Pieper, D., & Azevedo, F. (2022). Politicizing mask-wearing: Predicting the success of behavioral interventions among Republicans and Democrats in the U.S. Scientific Reports, 12, 7575.  doi.org/10.1038/s41598-022-10524-1\n\n\n\n2021\n\nGelfand, M. J., Li, R., Stamkou, E., Denison, E., Fernandez, J., Choi, V., Chatmen, J., Jackson, J. C., Pieper, D., & Dimant, E. (2021). Persuading conservatives and liberals to comply with mask-wearing: An intervention tournament. Journal of Experimental Social Psychology.  doi.org/10.1016/j.jesp.2022.104299\nGelfand, M. J., Jackson, J. C., Pan, X., Nau, D., Pieper, D., Denison, E., Dagher, M., Van Lange, P. A. M., Chiu, C., & Wang, M. (2021). The relationship between cultural tightness–looseness and COVID-19 cases and deaths: A global analysis. The Lancet: Planetary Health.  doi.org/10.1016/S2542-5196(20)30301-6"
  },
  {
    "objectID": "publications.html#masters-thesis",
    "href": "publications.html#masters-thesis",
    "title": "Publications",
    "section": " Master’s Thesis",
    "text": "Master’s Thesis\n\n\n2020\n\nPieper, D. J. (2020). Challenging social systems under the threat of pollution: Replication and extension of Eadeh and Chang (2019) [Master’s thesis, University of Northern Iowa]. UNI ScholarWorks.  scholarworks.uni.edu/etd/1028"
  }
]