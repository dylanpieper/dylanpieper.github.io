[
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Packages",
    "section": "",
    "text": "redquack transfers REDCap data to DuckDB with minimal memory overhead, designed for large datasets that exceed available RAM\n\n\n\nhellmer enables sequential or parallel batch processing for chat models supported by ellmer\n\n\n\nsamesies compares lists of texts, factors, or numerical values to measure their similarity\n\n\n\nshinysurveyjs integrates SurveyJS with Shiny to interface with a PostgreSQL database and create dynamic survey experiences"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBatch and Compare the Similarity of LLM Responses in R\n\n\n\n\n\n\npackages\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nDylan Pieper\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan Pieper",
    "section": "",
    "text": "I am a data scientist in the School of Pharmacy at the University of Pittsburgh.\nI am also a dog data üêæ, paragliding pilot ü™Ç, and hot yogi üßò.\nI‚Äôm happy to connect with you, dylanpieper@pitt.edu!"
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html",
    "href": "blog/posts/batch-and-compare-LLM-responses.html",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "",
    "text": "R is leading the way for data scientists to make the most of large language models. Posit‚Äôs official package ellmer provides a powerful functional interface for chatting, streaming responses, extracting data, and calling functions‚Äîbuilt on the modern, safe, and fast HTTP client httr2."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#introduction",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#introduction",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "",
    "text": "R is leading the way for data scientists to make the most of large language models. Posit‚Äôs official package ellmer provides a powerful functional interface for chatting, streaming responses, extracting data, and calling functions‚Äîbuilt on the modern, safe, and fast HTTP client httr2."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#batch-processing-options",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#batch-processing-options",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Batch Processing Options",
    "text": "Batch Processing Options\nData scientists often need to provide multiple prompts to LLMs to complete many actions at time‚Äîa process referred to as batch processing. Batch processing can be achieved at an API level for some providers (OpenAI, Anthropic, Gemini, and Mistral). This option is around 50% cheaper than requesting responses in real-time and delivers responses within 24 hours. See tidyllm for a CRAN package allows you to implement batch APIs.\nReal-time batching costs more, but it delivers immediate responses and supports any LLM provider. To achieve real-time batching, I created a robust wrapper package around ellmer called hellmer with a user-friendly interface and rich features such as tooling and structured data extraction, progress tracking and recovery, automatic retry with backoff, and more."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#sequential-vs-parallel-processing",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#sequential-vs-parallel-processing",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Sequential vs Parallel Processing",
    "text": "Sequential vs Parallel Processing\nThere are two methods you can use for batch processing: sequential or parallel processing. Sequential processing always uses one process and makes one API request at a time.\nSequential processing is slow, which could be good or bad depending on your goal, and generally safer because you can save your progress at each step.\nParallel processing is fast but makes it tricky to keep track of all of the responses. In hellmer, parallel processing allows chatting with LLMs at the same time across multiple R processes using the future framework. The chats are distributed across the processes in chunks (e.g., 50 prompts or chats). Once a chunk is finished, the responses are saved to the disk and returned once all of the chunks are complete."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#batch-with-hellmer",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#batch-with-hellmer",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Batch with hellmer",
    "text": "Batch with hellmer\nImplementing real-time batch processing in R is simple and only requires that you have your provider‚Äôs API key configured and the packages installed.\nI recommend setting your API keys for your LLM providers in my user or project environment using usethis::edit_r_environ(scope = c(\"user\", \"project\"), which will open a .Renviron file where you can add any API key, such as OPENAI_API_KEY=your-key or ANTHROPIC_API_KEY=your-key.\nInstall the package from CRAN: install.packages(\"hellmer\").\nThe two primary functions chat_sequential and chat_future create a sequential or parallel processor around an ellmer chat function. For example:\n\nlibrary(hellmer)\n\n# Option 1: Sequential processing\nchat &lt;- chat_sequential(chat_openai(system_prompt = \"Reply concisely\"))\n\n# Option 2: Parallel processing via future\nchat &lt;- chat_future(chat_openai(system_prompt = \"Reply concisely\"))\n\nresult &lt;- chat$batch(list(\n  \"What is R?\",\n  \"What is Python?\",\n  \"What is Julia?\",\n  \"What is Rust?\"\n))\n# Methods\nresult$progress() # Return batch progress (if interuppted)\nresult$texts() # Return list of responses\nresult$chats() # Return ellmer chat objects"
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#compare-the-similarity-of-llm-responses",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#compare-the-similarity-of-llm-responses",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Compare the similarity of LLM responses",
    "text": "Compare the similarity of LLM responses\nImagine I have a batch of LLM responses. I ask for the sentiment of texts (positive, neutral, or negative). Like a good scientist, I should manually review the responses to assess their validity. In true psychometric fashion, I may even ask research assistants or colleagues to independently review the responses to determine the inter-rater reliability. I would then analyze the ratings and resolve any conflicts.\nBut, what if, I could simulate this process using different LLM models or providers. I could ask OpenAI and Claude to complete the same task, compare their responses, and resolve any conflicts. This strategy is often referred to as LLM-as-a-judge; although, in this case, a more accurate term might be LLM-as-an-independent-rater.\nThat‚Äôs how I got the idea for samesies‚Äîa package I developed to compare lists of texts, factors, or numerical values to measure their similarity.\nInstall the package from CRAN: install.packages(\"samesies\").\nThe three primary functions (same_text, same_factor, and same_number) accept two or more lists as inputs, including nested listed, but are inherently typed and will not work with mixed types. For example:\n\n# Text\nr1 &lt;- list(\n  \"R is a statistical computing software\",\n  \"R enables grammar of graphics using ggplot2\",\n  \"R supports advanced statistical models\"\n)\nr2 &lt;- list(\n  \"R is a full-stack programming language\",\n  \"R enables advanced data visualizations\",\n  \"R supports machine learning algorithms\"\n)\ntex &lt;- same_text(r1, r2)\n\n# Factors\ncats1 &lt;- list(\"R\", \"R\", \"Python\")\ncats2 &lt;- list(\"R\", \"Python\", \"R\")\nfct &lt;- same_factor(cats1, cats2,\n  levels = c(\"R\", \"Python\")\n)\n\n# Numbers\nn1 &lt;- list(1, 2, 3)\nn2 &lt;- list(1, 2.1, 3.2)\nnum &lt;- same_number(n1, n2)"
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#complete-example",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#complete-example",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Complete example",
    "text": "Complete example\nI‚Äôll use a minimal example to demonstrate how to batch and evaluate the similarity of LLM responses in R using hellmer and samesies.\nI will use hellmer to chat with OpenAI (gpt-4o) and Claude (claude-3-5-sonnet-latest). I‚Äôll ask for the sentiment of our prompts as a:\n\ntext (emotional tone),\nfactor (positive, neutral, or negative), and\nnumber (0.0 to 1.0).\n\nBelow are my prompts and code to batch the LLM responses:\n\nlibrary(hellmer)\n\nprompts &lt;- list(\n  # Positive\n  \"R makes data visualization incredibly easy with ggplot2.\",\n  \"I love how R integrates statistics and data science seamlessly.\",\n  \"The R community is really supportive and welcoming.\",\n  # Neutral\n  \"R is commonly used in academic research.\",\n  \"R has both base functions and tidyverse functions for data manipulation.\",\n  \"RStudio is one of the most popular IDEs for R development.\",\n  # Negative\n  \"R is painfully slow for large datasets compared to Python.\",\n  \"R's object-oriented system is confusing and inconsistent.\",\n  \"Installing packages in R can be frustrating due to dependency errors.\",\n  # Ambiguous\n  \"I use R every day, but I'm not sure if I love it or hate it.\",\n  \"Tidyverse makes R more accessible, but it adds another layer of abstraction.\",\n  \"R has a steep learning curve, but once you get it, it's great.\"\n)\n\nopenai &lt;- chat_future(chat_openai())\nclaude &lt;- chat_future(chat_claude())\n\ntype_sentiment &lt;- type_object(\n  \"Extract sentiments\",\n  sentiment_str = type_string(\"Describe the emotional tone in one word\"),\n  sentiment_fct = type_enum(\"The sentiment type\", c(\"positive\", \"neutral\", \"negative\")),\n  sentiment_num = type_number(\"Negative to positive sentiment score, 0.00 to 1.00\"),\n)\n\nopenai_dat &lt;- openai$batch(prompts, type_spec = type_sentiment)\nclaude_dat &lt;- claude$batch(prompts, type_spec = type_sentiment)\n\nopenai_dat &lt;- openai_dat$structured_data()\nopenai_dat &lt;- list(\n  sentiment_str = purrr::map_chr(openai_dat, \"sentiment_str\"),\n  sentiment_fct = purrr::map_chr(openai_dat, \"sentiment_fct\"),\n  sentiment_num = purrr::map_dbl(openai_dat, \"sentiment_num\")\n)\n\nclaude_dat &lt;- claude_dat$structured_data()\nclaude_dat &lt;- list(\n  sentiment_str = purrr::map_chr(claude_dat, \"sentiment_str\"),\n  sentiment_fct = purrr::map_chr(claude_dat, \"sentiment_fct\"),\n  sentiment_num = purrr::map_dbl(claude_dat, \"sentiment_num\")\n)\n\nI wrangle the nested list that ellmer returns to a list of lists, because I will extract them based on type to run my samsies functions.\nFor those who are curious, here‚Äôs what the wrangled data looks like:\n\nopenai_dat\n# $sentiment_str\n#  [1] \"appreciative\"             \"appreciative\"             \"Positive and encouraging\" \"neutral\"\n#  [5] \"informative\"              \"Informative\"              \"Critical\"                 \"frustrated\"\n#  [9] \"frustration\"              \"ambivalent\"               \"Balanced\"                 \"Encouraging\"\n\n# $sentiment_fct\n#  [1] \"positive\" \"positive\" \"positive\" \"neutral\"  \"neutral\"  \"neutral\"  \"negative\" \"negative\" \"negative\" \"neutral\"  \"neutral\"\n#  [12] \"positive\"\n\n# $sentiment_num\n#  [1] 0.85 0.95 0.88 0.50 0.50 0.75 0.35 0.15 0.25 0.50 0.50 0.75\n\nclaude_dat\n# $sentiment_str\n#  [1] \"enthusiastic\" \"enthusiastic\" \"enthusiastic\" \"favorable\"    \"neutral\"      \"appreciative\" \"frustrated\"   \"frustrated\"\n#  [9] \"frustrated\"   \"ambivalent\"   \"ambivalent\"   \"optimistic\"\n\n# $sentiment_fct\n#  [1] \"positive\" \"positive\" \"positive\" \"positive\" \"neutral\"  \"positive\" \"negative\" \"negative\" \"negative\" \"neutral\"  \"neutral\"\n# [12] \"positive\"\n\n# $sentiment_num\n#  [1] 0.85 0.85 0.90 0.75 0.50 0.75 0.20 0.20 0.20 0.50 0.50 0.70\n\nI‚Äôm ready to compare the responses:\n\nlibrary(samesies)\n\ncheck_str &lt;- same_text(\n  \"openai\" = openai_dat$sentiment_str |&gt; as.list(),\n  \"claude\" = claude_dat$sentiment_str |&gt; as.list()\n)\naverage_similarity(check_str)\n#   osa      lv      dl hamming     lcs   qgram  cosine jaccard      jw soundex\n# 0.390   0.390   0.390   0.194   0.219   0.584   0.643   0.495   0.666   0.250\n\ncheck_fct &lt;- same_factor(\n  \"openai\" = openai_dat$sentiment_fct |&gt; as.list(),\n  \"claude\" = claude_dat$sentiment_fct |&gt; as.list(),\n  levels = c(\"positive\", \"neutral\", \"negative\")\n)\naverage_similarity(check_fct)\n# exact\n# 0.833\n\ncheck_num &lt;- same_number(\n  \"openai\" = openai_dat$sentiment_num |&gt; as.list(),\n  \"claude\" = claude_dat$sentiment_num |&gt; as.list()\n)\naverage_similarity(check_num)\n# exact        raw        exp    percent normalized      fuzzy\n# 0.417      0.056      0.948      0.883      0.930      0.950\n\nFrom this analysis, I learned that the two models have good agreement on the factor-based sentiment classification (83% exact match), excellent agreement on the numerical sentiment scores (93% normalized similarity), and moderate agreement on the text descriptions (64% cosine similarity). The lower agreement on text descriptions reflects the more subjective nature of choosing a single word to describe emotional tone."
  },
  {
    "objectID": "blog/posts/batch-and-compare-LLM-responses.html#conclusion",
    "href": "blog/posts/batch-and-compare-LLM-responses.html#conclusion",
    "title": "Batch and Compare the Similarity of LLM Responses in R",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with LLMs in R has evolved rapidly, and the combination of hellmer for efficient batching and samesies for response comparison creates a powerful toolkit for data scientists. These packages fill important gaps in the R ecosystem and enable more sophisticated LLM workflows.\nThe ability to validate LLM outputs against each other provides a level of quality assurance that‚Äôs essential for production systems. The example demonstrated that models tend to agree strongly on structured outputs, with more variation in free-text responses‚Äîa pattern that can guide how we design extraction prompts.\nLooking ahead, these tools will become increasingly important as organizations integrate LLMs into their data pipelines. The R ecosystem continues to lead in providing pragmatic solutions for working with AI, allowing data scientists to leverage these technologies without sacrificing reliability or interpretability.\nI hope that these packages can become part of the standard toolkit for anyone working with LLMs in R. The combination of efficient batching and systematic comparison provides the foundation for reliable, production-ready LLM implementations."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Dimant, E., Clemente, E. G., Pieper, D., & Azevedo, F. (2022). Politicizing mask-wearing: Predicting the success of behavioral interventions among Republicans and Democrats in the U.S. Scientific Reports, 12, 7575. https://doi.org/10.1038/s41598-022-10524-1\nGelfand, M. J., Li, R., Stamkou, E., Denison, E., Fernandez, J., Choi, V., Chatmen, J., Jackson, J. C., Pieper, D., & Dimant, E. (2021). Persuading conservatives and liberals to comply with mask-wearing: An intervention tournament. Journal of Experimental Social Psychology. https://doi.org/10.1016/j.jesp.2022.104299\nGelfand, M. J., Jackson, J. C., Pan, X., Nau, D., Pieper, D., Denison, E., Dagher, M., Van Lange, P. A. M., Chiu, C., & Wang, M. (2021). The relationship between cultural tightness‚Äìlooseness and COVID-19 cases and deaths: A global analysis. The Lancet: Planetary Health. https://doi.org/10.1016/S2542-5196(20)30301-6"
  },
  {
    "objectID": "publications.html#published",
    "href": "publications.html#published",
    "title": "Publications",
    "section": "",
    "text": "Dimant, E., Clemente, E. G., Pieper, D., & Azevedo, F. (2022). Politicizing mask-wearing: Predicting the success of behavioral interventions among Republicans and Democrats in the U.S. Scientific Reports, 12, 7575. https://doi.org/10.1038/s41598-022-10524-1\nGelfand, M. J., Li, R., Stamkou, E., Denison, E., Fernandez, J., Choi, V., Chatmen, J., Jackson, J. C., Pieper, D., & Dimant, E. (2021). Persuading conservatives and liberals to comply with mask-wearing: An intervention tournament. Journal of Experimental Social Psychology. https://doi.org/10.1016/j.jesp.2022.104299\nGelfand, M. J., Jackson, J. C., Pan, X., Nau, D., Pieper, D., Denison, E., Dagher, M., Van Lange, P. A. M., Chiu, C., & Wang, M. (2021). The relationship between cultural tightness‚Äìlooseness and COVID-19 cases and deaths: A global analysis. The Lancet: Planetary Health. https://doi.org/10.1016/S2542-5196(20)30301-6"
  },
  {
    "objectID": "publications.html#in-preparation",
    "href": "publications.html#in-preparation",
    "title": "Publications",
    "section": "In Preparation",
    "text": "In Preparation\nPieper, D. J., McNeill, J. M., Chirdon, C. R., Moore, D. W., Wang, E.-H., Raby, S., & Cloutier, R. M. Racial disparities in Pennsylvania‚Äôs criminal justice system: Analyzing prosecutorial and judicial decision-making. (In preparation)"
  },
  {
    "objectID": "publications.html#masters-thesis",
    "href": "publications.html#masters-thesis",
    "title": "Publications",
    "section": "Master‚Äôs Thesis",
    "text": "Master‚Äôs Thesis\nPieper, D. J. (2020). Challenging social systems under the threat of pollution: Replication and extension of Eadeh and Chang (2019) [Master‚Äôs thesis, University of Northern Iowa]. UNI ScholarWorks. https://scholarworks.uni.edu/etd/1028"
  }
]